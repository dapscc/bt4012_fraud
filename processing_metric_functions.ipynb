{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "from category_encoders.one_hot import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def get_processed_data():\n",
    "    ## Global data preprocessing\n",
    "    df = pd.read_csv(\"carclaims.csv\")\n",
    "    df = df.drop(columns=['PolicyNumber',\"PolicyType\", \"RepNumber\"])\n",
    "    df['Age'] =df['Age'].replace({0:16.5})\n",
    "    df = df[df[\"MonthClaimed\"]!='0']\n",
    "    \n",
    "    ## Feature Creation\n",
    "    # Weekend Feature\n",
    "    df[\"Weekend\"] = df[\"DayOfWeek\"].apply(lambda x: 1 if x in [\"Saturday\", \"Sunday\"] else 0)\n",
    "    \n",
    "    # Accident Prone age groups based on https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/810853\n",
    "    df[\"AccidentProneAge\"] = df[\"Age\"].apply(lambda x: 1 if (16 <= int(x) <= 25 | int(x) > 65) else 0)\n",
    "    \n",
    "    \n",
    "    ## Encoding ordinal features\n",
    "    col_ordering = [{'col':'Month','mapping':{'Jan':1,'Feb':2,'Mar':3,'Apr':4,'May':5,'Jun':6,'Jul':7,'Aug':8,'Sep':9,'Oct':10,'Nov':11,'Dec':12}},\n",
    "        {'col':'DayOfWeek','mapping':{'Monday':1,'Tuesday':2,'Wednesday':3,'Thursday':4,'Friday':5,'Saturday':6,'Sunday':7}},\n",
    "        {'col':'DayOfWeekClaimed','mapping':{'Monday':1,'Tuesday':2,'Wednesday':3,'Thursday':4,'Friday':5,'Saturday':6,'Sunday':7}},\n",
    "        {'col':'MonthClaimed','mapping':{'Jan':1,'Feb':2,'Mar':3,'Apr':4,'May':5,'Jun':6,'Jul':7,'Aug':8,'Sep':9,'Oct':10,'Nov':11,'Dec':12}},\n",
    "        {'col':'PastNumberOfClaims','mapping':{'none':0 ,'1':1,'2 to 4':2,'more than 4':5 }},\n",
    "        {'col':'NumberOfSuppliments','mapping':{'none':0,'1 to 2':1,'3 to 5':3,'more than 5':6}}, \n",
    "        {'col':'VehiclePrice','mapping':{'less than 20,000':1,'20,000 to 29,000':2,'30,000 to 39,000':3,\n",
    "                                        '40,000 to 59,000':4,'60,000 to 69,000':5, 'more than 69,000':6}},\n",
    "        {'col':'AgeOfVehicle','mapping':{'3 years':3,'6 years':6,'7 years':7,'more than 7':8,'5 years':5,'new':0,'4 years':4,'2 years':2}},\n",
    "        {'col':'Days:Policy-Accident','mapping':{'more than 30':4,'15 to 30':3,'none':0,'1 to 7':1,'8 to 15':2}},\n",
    "        {'col':'Days:Policy-Claim','mapping':{'more than 30':4,'15 to 30':3,'none':0,'1 to 7':1,'8 to 15':2}},\n",
    "        {'col':'AgeOfPolicyHolder','mapping':{'16 to 17':1,'18 to 20':2,'21 to 25':3,'26 to 30':4,'31 to 35':5,'36 to 40':6,\n",
    "                                            '41 to 50':7,'51 to 65':8,'over 65':9}},\n",
    "        {'col':'AddressChange-Claim','mapping':{'no change':0,'under 6 months':1,'1 year':2,'2 to 3 years':3,'4 to 8 years':4}},\n",
    "        {'col':'NumberOfCars','mapping':{'1 vehicle':1,'2 vehicles':2,'3 to 4':3,'5 to 8':4,'more than 8':5}}]\n",
    "\n",
    "\n",
    "\n",
    "    ord_encoder = OrdinalEncoder(mapping = col_ordering, return_df=True)\n",
    "    df2 = df.copy()\n",
    "    df2 = ord_encoder.fit_transform(df2)\n",
    "\n",
    "    ## Encoding nominal features\n",
    "    onehot = OneHotEncoder(cols=[\"Make\",'MaritalStatus', 'VehicleCategory', 'BasePolicy'], use_cat_names=True, return_df=True) \n",
    "    df3 = onehot.fit_transform(df2)\n",
    "\n",
    "    df4 = df3.copy()\n",
    "    df4[['PoliceReportFiled', 'WitnessPresent']] = df3[['PoliceReportFiled', 'WitnessPresent']].replace({'No': 0, 'Yes': 1})\n",
    "    df4[['AccidentArea']] = df4[['AccidentArea']].replace( {\n",
    "        'Rural' : 0,\n",
    "        'Urban' : 1\n",
    "        })\n",
    "    df4[['Fault']] = df4[['Fault']].replace( {\n",
    "        'Third Party' : 0,\n",
    "        'Policy Holder' : 1\n",
    "        })\n",
    "    df4[['Sex']] = df4[['Sex']].replace( {\n",
    "        'Female' : 0,\n",
    "        'Male' : 1\n",
    "        })\n",
    "    df4[['AgentType']] = df4[['AgentType']].replace({\n",
    "        'Internal' : 0,\n",
    "        'External' : 1\n",
    "        })\n",
    "    df4[['FraudFound']] = df4[['FraudFound']].replace({\n",
    "        'No' : 0,\n",
    "        'Yes' : 1\n",
    "        })\n",
    "\n",
    "    # df_binary_encoded = pd.get_dummies(df4['Make'], prefix='Make')\n",
    "    # df4 = pd.concat([df4, df_binary_encoded], axis=1)\n",
    "    # df4 = df4.drop('Make', axis=1)\n",
    "    \n",
    "    df4.to_csv('processed_data.csv', index=False)\n",
    "\n",
    "    X = df4.drop('FraudFound', axis=1)  # Features\n",
    "    y = df4['FraudFound']  # Target variable\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "    print(\"Validation set shape:\", X_val.shape, y_val.shape)\n",
    "    print(\"Test set shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return df4, X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
    "\n",
    "def undersample(X_train, y_train):\n",
    "    undersampled_data = RandomUnderSampler(sampling_strategy='majority', random_state=42)\n",
    "    X_under, y_under = undersampled_data.fit_resample(X_train, y_train)\n",
    "    return X_under, y_under\n",
    "\n",
    "def oversample(X_train, y_train):\n",
    "    oversampled_data = RandomOverSampler(sampling_strategy='minority', random_state=42)\n",
    "    X_over, y_over = oversampled_data.fit_resample(X_train, y_train)\n",
    "    return X_over, y_over\n",
    "\n",
    "def smote(X_train, y_train):\n",
    "    smote_data = SMOTE(random_state=42)\n",
    "    X_smote, y_smote = smote_data.fit_resample(X_train, y_train)\n",
    "    return X_smote, y_smote\n",
    "\n",
    "def ncr(X_train, y_train):\n",
    "    undersample = NeighbourhoodCleaningRule(n_neighbors=3, threshold_cleaning=0.5)\n",
    "    y_copy = y_train.copy()\n",
    "    y_copy = y_copy.replace(\"Yes\", 1)\n",
    "    y_copy = y_copy.replace(\"No\", 0)\n",
    "    X_ncr, y_ncr = undersample.fit_resample(X_train, y_copy)\n",
    "    # y_ncr = y_ncr.replace(\"Yes\", 1)\n",
    "    # y_ncr = y_ncr.replace(\"No\", 0)\n",
    "    # y_ncr = y_ncr.replace(1, \"Yes\")\n",
    "    # y_ncr = y_ncr.replace(0, \"No\")\n",
    "    return X_ncr, y_ncr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def forward_select(model, X_train, y_train):\n",
    "    model.fit(X_train, y_train)\n",
    "    ffs = SequentialFeatureSelector(model, k_features='best', forward=True, n_jobs=-1)\n",
    "    ffs.fit(X_train, y_train) \n",
    "    features = list(ffs.k_feature_names_)\n",
    "    print(f\"Features selected: {features}\")\n",
    "    return features\n",
    "\n",
    "def backward_select(model, X_train, y_train):\n",
    "    model.fit(X_train, y_train)\n",
    "    bfs = SequentialFeatureSelector(model, k_features='best', forward=False, n_jobs=-1)\n",
    "    bfs.fit(X_train, y_train) \n",
    "    features = list(bfs.k_feature_names_)\n",
    "    print(f\"Features selected: {features}\")\n",
    "    return features\n",
    "\n",
    "def rf_select(X_train, y_train):\n",
    "    sel = SelectFromModel(RandomForestClassifier(), threshold= \"0.5*mean\")\n",
    "    sel.fit(X_train, y_train)\n",
    "    selected_feat= X_train.columns[(sel.get_support())]\n",
    "    print(len(selected_feat))\n",
    "    print(selected_feat)\n",
    "    return selected_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chosen Metrics and Metric Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def show_metrics(actual, predicted, pos_label = 'Yes', neg_label = 'No'):\n",
    "    confusion_matrix = metrics.confusion_matrix(actual, predicted)\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "    cm_display.plot()\n",
    "    plt.show()\n",
    "    accuracy = metrics.accuracy_score(actual, predicted)\n",
    "    precision = metrics.precision_score(actual, predicted, pos_label = pos_label)\n",
    "    recall = metrics.recall_score(actual, predicted, pos_label = pos_label)\n",
    "    specificity = metrics.recall_score(actual, predicted, pos_label= neg_label)\n",
    "    f1_score = metrics.f1_score(actual, predicted, pos_label = pos_label)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Specificity: {specificity}\")\n",
    "    print(f\"F1_score: {f1_score}\")\n",
    "\n",
    "    # Generate ROC curve and calculate AUC\n",
    "    y_pred_classes = [1 if i>0.5 else 0 for i in predicted]\n",
    "    fpr, tpr, thresholds = roc_curve(actual, y_pred_classes)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {roc_auc:.2f}')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_metrics(actual, predicted, pos_label = 'Yes', neg_label = 'No'):\n",
    "\n",
    "    accuracy = metrics.accuracy_score(actual, predicted)\n",
    "    precision = metrics.precision_score(actual, predicted, pos_label = pos_label)\n",
    "    recall = metrics.recall_score(actual, predicted, pos_label = pos_label)\n",
    "    specificity = metrics.recall_score(actual, predicted, pos_label= neg_label)\n",
    "    f1_score = metrics.f1_score(actual, predicted, pos_label = pos_label)\n",
    "\n",
    "    return accuracy, precision, recall, specificity, f1_score\n",
    "\n",
    "### Can ignore but don't remove\n",
    "def show_metrics_DL(actual, predicted, samp, pos_label = 1, neg_label = 0):\n",
    "    confusion_matrix = metrics.confusion_matrix(actual, predicted)\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "    cm_display.plot()\n",
    "    plt.show()\n",
    "    accuracy = metrics.accuracy_score(actual, predicted)\n",
    "    precision = metrics.precision_score(actual, predicted, pos_label = pos_label)\n",
    "    recall = metrics.recall_score(actual, predicted, pos_label = pos_label)\n",
    "    specificity = metrics.recall_score(actual, predicted, pos_label= neg_label)\n",
    "    f1_score = metrics.f1_score(actual, predicted, pos_label = pos_label)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Specificity: {specificity}\")\n",
    "    print(f\"F1_score: {f1_score}\")\n",
    "\n",
    "    # Generate ROC curve and calculate AUC\n",
    "    y_pred_classes = [1 if i>0.5 else 0 for i in predicted]\n",
    "    fpr, tpr, thresholds = roc_curve(actual, y_pred_classes)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {roc_auc:.2f}')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve ({samp})')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
