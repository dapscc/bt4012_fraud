{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few shot learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the limited amount of positive (fraudulent) data points available, we explored few-shot learning, which is a method that specialises in training with small training data. We implemented a classifier using prototypical networks as the underlying architecture. \n",
    "\n",
    "We hypothesized that a few shot classifier would be able to perform comparably to the previous supervised leraning models, while using less samples. To validate this, we will evaluate the classifier's performance on different values of k, where k is the number of examples of each class that is seen by the model during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from get_processed_data import get_processed_data\n",
    "from FSLMethods import form_datasets\n",
    "from FSLTrainer import FSLTrainer\n",
    "from show_metrics import show_metrics\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Test-Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, X_train, y_train, X_val, y_val, X_test, y_test = get_processed_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training (meta-learning / episodic training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Episodic training simulates the few-shot learning scenario to train a prototypical network. Training data is organized into episodes that resemble few-shot tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we determine whether to do feature selection, which method to sample with (if any), and what size the embedding of the prototypical network should be. k is initialized as a constant 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_1 = {\n",
    "    'n_shot': [10],\n",
    "    'embedding_size': [2 ** x for x in range(2, 6)] ## Try {4, 8, 16, 32}\n",
    "}\n",
    "results_1 = {} ## key:value = (feature_selection, sampling_method):(recall, f1-score)\n",
    "idx = 0\n",
    "\n",
    "for feature_selection in [True, False]:\n",
    "    for sampling_method in ['', 'oversampling', 'undersampling', 'smote']:\n",
    "        print(f'##### Run {idx} #####')\n",
    "        print(f'Feature selection: {feature_selection}, sampling method: {sampling_method}')\n",
    "        train_set, validation_set, test_set = \\\n",
    "            form_datasets(X_train, y_train, X_val, y_val, X_test, y_test, \n",
    "                          feature_selection = feature_selection, sampling_method = sampling_method)\n",
    "        trainer_1 = FSLTrainer(train_set, validation_set, test_set, config_1)\n",
    "\n",
    "        curr_results, best_config = trainer_1.tune(metric = 'recall')\n",
    "        print(f'Precision: {curr_results[best_config][0].precision}, F1: {curr_results[best_config][0].f1_score}, AUC: {curr_results[best_config][0].auc}')\n",
    "        idx += 1\n",
    "        # print(f'Results: Recall = {curr_results[best_config][0]}, best embedding size = {best_config[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, the best recall was obtained by the model that used feature selection and sampling with SMOTE, and had an embedding size of 8 (ie the feature extractor of the prototypical network embeds inputs into vectors of size 8). We will now experiment with various values of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, validation_set, test_set = \\\n",
    "    form_datasets(X_train, y_train, X_val, y_val, X_test, y_test, \n",
    "                  feature_selection = True, sampling_method = 'smote')\n",
    "\n",
    "config_2 = {\n",
    "    'n_shot': [4, 8, 16, 24, 32, 48, 64],\n",
    "    'embedding_size': [32]\n",
    "}\n",
    "\n",
    "trainer_2 = FSLTrainer(train_set, validation_set, test_set, config_2)\n",
    "results, best_config = trainer_2.tune(metric = 'recall') ## Key:Value = (k, embedding_size):(metric, model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we will take the optimal k to be 48, and the optimal embedding size to be 8. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now determine the minimum k required to match the performance of the supervised learning models. This is done by evaluating the previously trained models on the testing set. Based on the performance of our other models, a threshold of 0.75 for the recall has been set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get trained models for each k\n",
    "threshold = 0.7\n",
    "temp_list = [(k, embedding_size, metric, model_params) for (k, embedding_size), (metric, model_params) in results.items()]\n",
    "temp_list = sorted(temp_list, key = lambda x: x[0])\n",
    "\n",
    "relevant_actuals = []\n",
    "relevant_predictions = []\n",
    "\n",
    "for curr_k, curr_size, _, curr_params in temp_list:\n",
    "    curr_config = {\n",
    "        'n_shot': curr_k,\n",
    "        'embedding_size': curr_size\n",
    "    }\n",
    "    test_metrics = trainer_2.test(curr_params, curr_config)\n",
    "    if test_metrics.recall > threshold:\n",
    "        print(f'Minimum k required to match performance threshold = {curr_k}')\n",
    "        show_metrics(actual = test_metrics.actuals, predicted = test_metrics.predictions, pos_label = 1, neg_label = 0)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bt4012",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
