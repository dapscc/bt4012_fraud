{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from easyfsl.samplers import TaskSampler\n",
    "from easyfsl.utils import evaluate\n",
    "from easyfsl.methods import FewShotClassifier, PrototypicalNetworks\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "from get_processed_data import get_processed_data\n",
    "from FSLMethods import form_datasets, training_epoch, evaluate_model\n",
    "from FSLDataset import FSLDataset\n",
    "from FSLNetworks import DummyNetwork\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (12335, 55) (12335,)\n",
      "Validation set shape: (1542, 55) (1542,)\n",
      "Test set shape: (1542, 55) (1542,)\n",
      "Disclaimer: It is assumed that the label is the last column of the input dataframe... ...\n",
      "Disclaimer: It is assumed that the label is the last column of the input dataframe... ...\n"
     ]
    }
   ],
   "source": [
    "df, X_train, y_train, X_val, y_val, X_test, y_test = get_processed_data()\n",
    "\n",
    "## Datasets need to be a FewShotDataset / torch Dataset with .get_labels\n",
    "train_set, validation_set = form_datasets(X_train, y_train, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training (meta-learning / episodic training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Episodic training simulates the few-shot learning scenario to train a prototypical network. Training data is organized into episodes that resemble few-shot tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each task contains N_WAY * (N_SHOT + N_QUERY) samples\n",
    "N_WAY = 2\n",
    "N_SHOT = 2\n",
    "N_QUERY = 3\n",
    "\n",
    "N_TASKS_PER_EPOCH = 10 \n",
    "N_VALIDATION_TASKS = 100 \n",
    "\n",
    "## Sampliers used to generate tasks\n",
    "train_sampler = TaskSampler(dataset = train_set, n_way = N_WAY, n_shot = N_SHOT, \n",
    "                            n_query = N_QUERY, n_tasks = N_TASKS_PER_EPOCH)\n",
    "validation_sampler = TaskSampler(dataset = validation_set, n_way = N_WAY, n_shot = N_SHOT,\n",
    "                                 n_query = N_QUERY, n_tasks = N_VALIDATION_TASKS)\n",
    "\n",
    "## Loader generates an iterable given a dataset and a sampler\n",
    "train_loader = DataLoader(dataset = train_set, batch_sampler = train_sampler, pin_memory = True,\n",
    "                          collate_fn = train_sampler.episodic_collate_fn)\n",
    "validation_loader = DataLoader(dataset = validation_set, batch_sampler = validation_sampler, pin_memory = True,\n",
    "                               collate_fn = validation_sampler.episodic_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing optimizer, loss function, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss fn\n",
    "LOSS_FN = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "## Scheduler\n",
    "    ## Scales learning rate by gamma at the designated milestones\n",
    "scheduler_milestones = [70, 85]\n",
    "scheduler_gamma = 0.1\n",
    "\n",
    "\n",
    "## Optimizer\n",
    "backbone = DummyNetwork(in_dim = len(X_train.columns), hidden_dim = 256, out_dim = 4) ## TODO: What should the correct dimensions be?\n",
    "model = PrototypicalNetworks(backbone, use_softmax = True)\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "MOMENTUM = 0.9\n",
    "DECAY = 5e-4\n",
    "train_optimizer = optim.SGD(params = model.parameters(), lr = LEARNING_RATE, momentum = MOMENTUM, \n",
    "                            weight_decay = DECAY)\n",
    "train_scheduler = MultiStepLR(optimizer = train_optimizer, milestones = scheduler_milestones,\n",
    "                              gamma = scheduler_gamma)\n",
    "\n",
    "\n",
    "## Writer\n",
    "log_dir = 'fsl_logs'\n",
    "tb_writer = SummaryWriter(log_dir = log_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 100\n",
    "log_update_frequency = 10\n",
    "\n",
    "## Track best parameters (weights and biases) and performance of model\n",
    "best_state = model.state_dict()\n",
    "best_f1_score = 0.0\n",
    "best_recall = 0.0\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f'Epoch: {epoch}')\n",
    "    \n",
    "    average_epoch_loss = training_epoch(model, train_loader, train_optimizer, LOSS_FN)\n",
    "\n",
    "    actuals, predictions, _, _, recall, _, f1_score = evaluate_model(model, validation_loader)\n",
    "    \n",
    "    if f1_score > best_f1_score:\n",
    "        best_f1_score = f1_score\n",
    "        # best_state = model.state_dict()\n",
    "        # print(\"Ding ding ding! We found a new best model!\")\n",
    "\n",
    "    if recall > best_recall:\n",
    "        best_recall = recall\n",
    "        best_state = model.state_dict()\n",
    "        print(\"Ding ding ding! We found a new best model!\")\n",
    "\n",
    "    tb_writer.add_scalar(\"Train/loss\", average_epoch_loss, epoch)\n",
    "    tb_writer.add_scalar('F1', f1_score, epoch)\n",
    "    tb_writer.add_scalar('Recall', recall, epoch)\n",
    "\n",
    "    ## Update the scheduler such that it knows when to adjust the learning rate\n",
    "    train_scheduler.step()\n",
    "\n",
    "\n",
    "## Retrieve the best model\n",
    "missing_keys, unexpected_keys = model.load_state_dict(best_state)\n",
    "print(f'Best f1-score after {N_EPOCHS} epochs of training: {best_f1_score}')\n",
    "print(f'Best recall after {N_EPOCHS} epochs of training: {best_recall}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(model, test_loader) ##TODO: Implement method"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bt4012",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
